# See docs/ci/AGENTS_POLICY.md for guardrails and override process.
name: Agents Auto-Pilot

# End-to-end automation: Issue ‚Üí Format ‚Üí Optimize ‚Üí Apply ‚Üí Agent ‚Üí Keepalive ‚Üí Merge
# Triggered by:
# 1. agents:auto-pilot label (initial trigger)
# 2. Issue/PR closed events (for verification)
# 3. Manual dispatch (for testing/recovery)

on:
  issues:
    types: [labeled, closed]
  pull_request:
    types: [labeled, closed]
  # Note: Optimizer now runs inline in auto-pilot, not as separate workflow
  # workflow_run trigger removed to eliminate race conditions
  # If needed for other child workflows, add them here specifically
  workflow_dispatch:
    inputs:
      issue_number:
        description: "Issue number to auto-pilot"
        required: true
        type: number
      force_step:
        description: "Force a specific step (optional, leave as 'auto' for normal flow)"
        required: false
        type: choice
        options:
          - auto
          - format
          - optimize
          - apply
          - capability-check
          - agent
          - verify

permissions:
  contents: read
  issues: write
  pull-requests: write  # Needed to create PRs automatically
  actions: write  # Needed for workflow re-dispatch

env:
  # Safety limits
  MAX_CYCLES: 10
  MAX_WALL_TIME_HOURS: 4
  AUTOPILOT_METRICS_LOG_PATH: .agents/autopilot-metrics.ndjson
  AUTOPILOT_METRICS_SUMMARY_PATH: .agents/autopilot-metrics-summary.ndjson

jobs:
  auto-pilot:
    runs-on: ubuntu-latest
    timeout-minutes: 240  # 4 hours = MAX_WALL_TIME_HOURS
    # Trigger on:
    # 1. agents:auto-pilot label added (starts the automation)
    # 2. Issue/PR closed (verification trigger)
    # 3. workflow_dispatch (manual trigger)
    # NOTE: Optimizer and formatter now run INLINE, not as separate workflows
    # This eliminates race conditions and workflow_run complexity
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event.action == 'labeled' && (
        github.event.label.name == 'agents:auto-pilot' ||
        github.event.label.name == 'agent:codex'
      )) ||
      (github.event.action == 'closed' &&
       contains(github.event.issue.labels.*.name, 'agents:auto-pilot'))

    steps:
      - name: Check if auto-pilot is enabled
        id: check_enabled
        uses: actions/github-script@v8
        with:
          script: |
            // Handle different event types
            if (context.eventName === 'workflow_dispatch') {
              core.setOutput('enabled', 'true');
              return;
            }

            // For label events, verify auto-pilot is enabled
            const labelName = context.payload.label?.name || '';

            // Get issue/PR to check for agents:auto-pilot label
            let labels = [];
            if (context.payload.issue) {
              labels = context.payload.issue.labels.map(l => l.name);
            } else if (context.payload.pull_request) {
              labels = context.payload.pull_request.labels.map(l => l.name);
            }

            const hasAutoPilot = labels.includes('agents:auto-pilot');
            if (!hasAutoPilot && labelName !== 'agents:auto-pilot') {
              core.info(`Skipping: auto-pilot not enabled (trigger: ${labelName})`);
              core.setOutput('enabled', 'false');
              return;
            }

            core.setOutput('enabled', 'true');

      - name: Checkout repository
        if: steps.check_enabled.outputs.enabled == 'true'
        uses: actions/checkout@v6

      - name: Set up Python
        if: steps.check_enabled.outputs.enabled == 'true'
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        if: steps.check_enabled.outputs.enabled == 'true'
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # Install langchain dependencies for inline optimizer
          pip install langchain langchain-core langchain-openai langchain-community

      - name: Initialize auto-pilot metrics logs
        if: steps.check_enabled.outputs.enabled == 'true'
        run: |
          mkdir -p .agents

      - name: Determine context
        if: steps.check_enabled.outputs.enabled == 'true'
        id: context
        uses: actions/github-script@v8
        with:
          script: |
            const { withRetry, paginateWithRetry } = require(
              './.github/scripts/github-api-with-retry.js'
            );

            let issueNumber, issue, pr;

            // Get issue number from various sources (in priority order)
            if (context.eventName === 'workflow_dispatch') {
              issueNumber = parseInt('${{ inputs.issue_number }}');
            } else if (context.payload.issue) {
              issueNumber = context.payload.issue.number;
              issue = context.payload.issue;
            } else if (context.payload.pull_request) {
              // For PR events, find linked issue
              pr = context.payload.pull_request;
              const bodyMatch = pr.body?.match(/#(\d+)/);
              issueNumber = bodyMatch ? parseInt(bodyMatch[1]) : null;
            }

            if (!issueNumber) {
              core.setFailed('Could not determine issue number');
              return;
            }

            // Fetch issue if not in payload (with retry)
            if (!issue) {
              const { data } = await withRetry(() =>
                github.rest.issues.get({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issueNumber
                })
              );
              issue = data;
            }

            const labels = issue.labels.map(l => l.name);

            // Check for pause label
            if (labels.includes('agents:auto-pilot-pause')) {
              core.info('Auto-pilot paused by agents:auto-pilot-pause label');
              core.setOutput('should_continue', 'false');
              core.setOutput('reason', 'paused');
              return;
            }

            // Check for failure/needs-human
            if (labels.includes('needs-human') || labels.includes('agents:auto-pilot-failed')) {
              core.info('Auto-pilot stopped: requires human intervention');
              core.setOutput('should_continue', 'false');
              core.setOutput('reason', 'needs-human');
              return;
            }

            // Determine current state based on labels
            // NOTE: Use 'agents:formatted' (completion marker) not 'agents:format' (old trigger)
            const hasFormat = labels.includes('agents:formatted');
            const hasOptimize = labels.includes('agents:optimize');
            const hasApplySuggestions = labels.includes('agents:apply-suggestions');
            const hasAgentCodex = labels.includes('agent:codex');
            const hasAutofix = labels.includes('autofix');
            const hasAutomerge = labels.includes('automerge');
            const hasVerify = labels.includes('verify:evaluate');

            // Check for actual optimizer output (not just label)
            // Since we run optimizer inline now, this should always exist after optimize step
            // the optimizer workflow actually completed its work
            const comments = await paginateWithRetry(
              github,
              github.rest.issues.listComments,
              {
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                per_page: 100
              }
            );

            const hasOptimizerOutput = comments.some(c => {
              const body = c.body || '';
              return body.includes('Issue Optimization Suggestions') ||
                     body.includes('ISSUE_OPTIMIZER_SUGGESTIONS') ||
                     body.includes('## üìã Optimization Suggestions');
            });

            // Check for linked PR (with pagination and multiple event types)
            const timelineEvents = await paginateWithRetry(
              github,
              github.rest.issues.listEventsForTimeline,
              {
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                per_page: 100
              }
            );

            let linkedPR = null;
            for (const event of timelineEvents) {
              // Handle both cross-referenced and connected events
              if ((event.event === 'cross-referenced' || event.event === 'connected') &&
                  event.source?.issue?.pull_request) {
                linkedPR = event.source.issue.number;
              }
            }

            core.setOutput('issue_number', issueNumber);
            core.setOutput('issue_title', issue.title);
            core.setOutput('issue_state', issue.state);
            core.setOutput('should_continue', 'true');
            core.setOutput('has_format', hasFormat.toString());
            core.setOutput('has_optimize', hasOptimize.toString());
            core.setOutput('has_optimizer_output', hasOptimizerOutput.toString());
            core.setOutput('has_apply', hasApplySuggestions.toString());
            core.setOutput('has_agent', hasAgentCodex.toString());
            core.setOutput('has_autofix', hasAutofix.toString());
            core.setOutput('has_automerge', hasAutomerge.toString());
            core.setOutput('has_verify', hasVerify.toString());
            core.setOutput('linked_pr', linkedPR || '');

            console.log(`Issue #${issueNumber} state:`);
            console.log(`  State: ${issue.state}`);
            console.log(`  Labels: ${labels.join(', ')}`);
            console.log(`  Optimizer output: ${hasOptimizerOutput}`);
            console.log(`  Linked PR: ${linkedPR || 'none'}`);

      - name: Check step count
        if: steps.context.outputs.should_continue == 'true'
        id: cycles
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
        with:
          script: |
            const { paginateWithRetry } = require('./.github/scripts/github-api-with-retry.js');
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);

            // Get all comments to count auto-pilot steps (with pagination and retry)
            const allComments = await paginateWithRetry(
              github,
              github.rest.issues.listComments,
              {
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                per_page: 100
              }
            );

            const stepComments = allComments.filter(c =>
              typeof c.body === 'string' && c.body.includes('ü§ñ Auto-pilot step')
            );

            const stepCount = stepComments.length;
            const maxCycles = parseInt('${{ env.MAX_CYCLES }}');

            if (stepCount >= maxCycles) {
              core.warning(`Auto-pilot exceeded max steps (${stepCount}/${maxCycles})`);
              core.setOutput('exceeded', 'true');
              return;
            }

            core.setOutput('exceeded', 'false');
            core.setOutput('count', stepCount.toString());

      - name: Stop if exceeded
        if: steps.cycles.outputs.exceeded == 'true'
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
        with:
          script: |
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);

            // Add failure label and comment
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              labels: ['needs-human', 'agents:auto-pilot-failed']
            });

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              body: `## ‚ö†Ô∏è Auto-Pilot Stopped

            **Reason:** Exceeded maximum cycle limit (${{ env.MAX_CYCLES }} cycles)

            This issue requires human review. Possible causes:
            - Repeated CI failures
            - Conflicting requirements
            - External dependencies

            **To resume:** Remove \`agents:auto-pilot-failed\` and
            \`needs-human\` labels, then re-add \`agents:auto-pilot\`.`
            });

            core.setFailed('Auto-pilot exceeded max cycles');

      - name: Determine next step
        if: |
          steps.context.outputs.should_continue == 'true' &&
          steps.cycles.outputs.exceeded != 'true'
        id: next
        env:
          FORCE_STEP: ${{ inputs.force_step }}
        run: |
          # Priority order of steps
          # 1. If issue closed and has verify label ‚Üí done
          # 2. If issue closed without verify ‚Üí add verify
          # 3. If no PR ‚Üí need agent assignment
          # 4. If has PR ‚Üí check PR state
          # 5. If no format ‚Üí format first
          # 6. If no optimize label ‚Üí trigger optimize
          # 7. If optimize label but no output ‚Üí wait (optimizer still running)
          # 8. If has optimizer output but no apply ‚Üí apply suggestions

          ISSUE_STATE="${{ steps.context.outputs.issue_state }}"
          HAS_FORMAT="${{ steps.context.outputs.has_format }}"
          HAS_OPTIMIZE="${{ steps.context.outputs.has_optimize }}"
          HAS_OPTIMIZER_OUTPUT="${{ steps.context.outputs.has_optimizer_output }}"
          HAS_APPLY="${{ steps.context.outputs.has_apply }}"
          HAS_AGENT="${{ steps.context.outputs.has_agent }}"
          LINKED_PR="${{ steps.context.outputs.linked_pr }}"
          HAS_VERIFY="${{ steps.context.outputs.has_verify }}"

          # Force step if specified (not 'auto')
          if [[ -n "$FORCE_STEP" && "$FORCE_STEP" != "auto" ]]; then
            echo "next_step=$FORCE_STEP" >> "$GITHUB_OUTPUT"
            echo "Forced step: $FORCE_STEP"
            exit 0
          fi

          # Issue closed = done or verify
          if [[ "$ISSUE_STATE" == "closed" ]]; then
            if [[ "$HAS_VERIFY" == "true" ]]; then
              echo "next_step=done" >> "$GITHUB_OUTPUT"
              echo "Issue closed with verification - auto-pilot complete"
            else
              echo "next_step=verify" >> "$GITHUB_OUTPUT"
              echo "Issue closed - triggering verification"
            fi
            exit 0
          fi

          # No PR yet - need to go through issue prep pipeline
          if [[ -z "$LINKED_PR" ]]; then
            if [[ "$HAS_FORMAT" != "true" ]]; then
              echo "next_step=format" >> "$GITHUB_OUTPUT"
              echo "Step 1: Format issue"
            elif [[ "$HAS_OPTIMIZER_OUTPUT" != "true" ]]; then
              # Run optimizer inline (not via label)
              echo "next_step=optimize" >> "$GITHUB_OUTPUT"
              echo "Step 2: Run optimizer (inline)"
            elif [[ "$HAS_APPLY" != "true" ]]; then
              echo "next_step=apply" >> "$GITHUB_OUTPUT"
              echo "Step 3: Apply suggestions"
            elif [[ "$HAS_AGENT" != "true" ]]; then
              echo "next_step=capability-check" >> "$GITHUB_OUTPUT"
              echo "Step 4: Run capability check and assign agent"
            else
              echo "next_step=create-pr" >> "$GITHUB_OUTPUT"
              echo "Step 5: All prep complete, checking for branch to create PR"
            fi
            exit 0
          fi

          # Has PR - check if it's complete and ready to merge
          # Get PR state to see if keepalive has finished
          if ! PR_STATE=$(gh api "/repos/${{ github.repository }}/issues/$LINKED_PR/comments" \
            --jq '.[] | select(.body | contains("keepalive-state:")) | .body' 2>&1 | tail -1); then
            echo "‚ö†Ô∏è Failed to fetch PR state, defaulting to monitor-pr"
            echo "next_step=monitor-pr" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Check if we got any state data
          if [ -z "$PR_STATE" ]; then
            echo "No keepalive state found, defaulting to monitor-pr"
            echo "next_step=monitor-pr" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Check if keepalive marked tasks as complete
          if echo "$PR_STATE" | grep -q '"last_action":"stop"' && \
             echo "$PR_STATE" | grep -q '"last_reason":"tasks-complete"'; then
            echo "next_step=check-completion" >> "$GITHUB_OUTPUT"
            echo "Step 6.5: PR tasks complete - checking for merge"
          else
            echo "next_step=monitor-pr" >> "$GITHUB_OUTPUT"
            echo "PR #$LINKED_PR exists - monitoring via keepalive"
          fi
          exit 0

      - name: Metrics - Start format timer
        if: steps.next.outputs.next_step == 'format'
        env:
          AUTOPILOT_STEP_NAME: format
          AUTOPILOT_ERROR_CATEGORY: timer-start
        run: |
          python scripts/autopilot_step_timer.py --event start --format epoch-ms --github-env

      - name: Execute step - Format (inline)
        if: steps.next.outputs.next_step == 'format'
        id: format_step
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
          STEP_COUNT: ${{ steps.cycles.outputs.count }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          ISSUE_NUMBER="$ISSUE_NUMBER"
          STEP_COUNT="$STEP_COUNT"

          # Post progress comment
          gh issue comment "${ISSUE_NUMBER}" --body \
            "ü§ñ **Auto-pilot step $((STEP_COUNT + 1))**: Starting issue formatting...

          Running formatter inline (not via label trigger)."

          # Get issue body
          gh api "repos/${{ github.repository }}/issues/${ISSUE_NUMBER}" > /tmp/issue.json
          jq -r '.body' /tmp/issue.json > /tmp/issue_body.md

          # Format the issue
          echo "Formatting issue into AGENT_ISSUE_TEMPLATE structure..."
          python scripts/langchain/issue_formatter.py \
            --input-file /tmp/issue_body.md \
            --json > /tmp/format_result.json

          # Extract and apply formatted body
          python -c "
          import json
          with open('/tmp/format_result.json') as f:
              result = json.load(f)
          formatted = result.get('formatted_body', '')
          if not formatted:
              print('ERROR: No formatted body returned')
              import sys
              sys.exit(1)
          with open('/tmp/formatted_body.md', 'w') as f:
              f.write(formatted)
          print('Issue formatted successfully')
          " || exit 1

          # Update issue body
          gh issue edit "${ISSUE_NUMBER}" --body-file /tmp/formatted_body.md

          # Add marker labels (but don't trigger workflow)
          gh issue edit "${ISSUE_NUMBER}" --add-label "agents:formatted" || true
          gh issue edit "${ISSUE_NUMBER}" --add-label "agents:apply-suggestions" || true

          echo "‚úÖ Formatting complete - continuing to next step"

      - name: Metrics - End format timer
        if: always() && steps.next.outputs.next_step == 'format'
        env:
          AUTOPILOT_STEP_NAME: format
          AUTOPILOT_ERROR_CATEGORY: timer-end
        run: |
          python scripts/autopilot_step_timer.py --event end --format epoch-ms --github-env

      - name: Metrics - Record format step
        if: always() && steps.next.outputs.next_step == 'format'
        env:
          AUTOPILOT_STEP_NAME: format
          AUTOPILOT_ERROR_CATEGORY: metrics-collector
          AUTOPILOT_METRICS_LOG_PATH: ${{ env.AUTOPILOT_METRICS_LOG_PATH }}
        run: |
          success="${{ steps.format_step.outcome == 'success' }}"
          failure_reason="none"
          if [ "$success" != "true" ]; then
            failure_reason="step-failed"
          fi
          python scripts/autopilot_metrics_collector.py \
            --path "$AUTOPILOT_METRICS_LOG_PATH" \
            --metric-type step \
            --issue-number "${{ steps.context.outputs.issue_number }}" \
            --cycle-count "${{ steps.cycles.outputs.count }}" \
            --step-name "format" \
            --success "$success" \
            --failure-reason "$failure_reason"

      - name: Metrics - Start optimize timer
        if: steps.next.outputs.next_step == 'optimize'
        env:
          AUTOPILOT_STEP_NAME: optimize
          AUTOPILOT_ERROR_CATEGORY: timer-start
        run: |
          python scripts/autopilot_step_timer.py --event start --format epoch-ms --github-env

      - name: Execute step - Optimize (inline)
        if: steps.next.outputs.next_step == 'optimize'
        id: optimize_step
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
          STEP_COUNT: ${{ steps.cycles.outputs.count }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          ISSUE_NUMBER="$ISSUE_NUMBER"
          STEP_COUNT="$STEP_COUNT"

          # Post progress comment
          gh issue comment "${ISSUE_NUMBER}" --body \
            "ü§ñ **Auto-pilot step $((STEP_COUNT + 1))**: Analyzing issue for improvements...

          Running optimizer inline (not via label trigger)."

          # Get issue body
          gh api "repos/${{ github.repository }}/issues/${ISSUE_NUMBER}" > /tmp/issue.json
          jq -r '.body' /tmp/issue.json > /tmp/issue_body.md

          # Run optimizer analysis
          echo "Running optimization analysis..."
          python scripts/langchain/issue_optimizer.py \
            --input-file /tmp/issue_body.md \
            --json > /tmp/suggestions.json

          # Format and post comment
          python -c "
          import json
          import sys
          sys.path.insert(0, 'scripts/langchain')
          from issue_optimizer import IssueOptimizationResult, format_suggestions_comment

          with open('/tmp/suggestions.json') as f:
              data = json.load(f)

          result = IssueOptimizationResult(
              task_splitting=data.get('task_splitting', []),
              blocked_tasks=data.get('blocked_tasks', []),
              objective_criteria=data.get('objective_criteria', []),
              missing_sections=data.get('missing_sections', []),
              formatting_issues=data.get('formatting_issues', []),
              overall_notes=data.get('overall_notes', ''),
              provider_used=data.get('provider_used')
          )

          comment = format_suggestions_comment(result)
          with open('/tmp/comment.md', 'w') as f:
              f.write(comment)
          " || {
            echo "Failed to format comment, using raw JSON"
            cat /tmp/suggestions.json > /tmp/comment.md
          }

          # Post suggestions comment
          gh issue comment "${ISSUE_NUMBER}" --body-file /tmp/comment.md

          # Add marker labels (but don't trigger workflow)
          gh issue edit "${ISSUE_NUMBER}" --add-label "agents:formatted" || true
          gh issue edit "${ISSUE_NUMBER}" --add-label "agents:apply-suggestions" || true

          echo "‚úÖ Optimization complete - continuing to next step"

      - name: Metrics - End optimize timer
        if: always() && steps.next.outputs.next_step == 'optimize'
        env:
          AUTOPILOT_STEP_NAME: optimize
          AUTOPILOT_ERROR_CATEGORY: timer-end
        run: |
          python scripts/autopilot_step_timer.py --event end --format epoch-ms --github-env

      - name: Metrics - Record optimize step
        if: always() && steps.next.outputs.next_step == 'optimize'
        env:
          AUTOPILOT_STEP_NAME: optimize
          AUTOPILOT_ERROR_CATEGORY: metrics-collector
          AUTOPILOT_METRICS_LOG_PATH: ${{ env.AUTOPILOT_METRICS_LOG_PATH }}
        run: |
          success="${{ steps.optimize_step.outcome == 'success' }}"
          failure_reason="none"
          if [ "$success" != "true" ]; then
            failure_reason="step-failed"
          fi
          python scripts/autopilot_metrics_collector.py \
            --path "$AUTOPILOT_METRICS_LOG_PATH" \
            --metric-type step \
            --issue-number "${{ steps.context.outputs.issue_number }}" \
            --cycle-count "${{ steps.cycles.outputs.count }}" \
            --step-name "optimize" \
            --success "$success" \
            --failure-reason "$failure_reason"

      - name: Guard - Require optimizer output before apply
        if: steps.next.outputs.next_step == 'apply'
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
        with:
          script: |
            // Guard: Verify optimizer produced suggestions before applying
            // This prevents silent failures when optimizer runs but produces no output
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);

            // Fetch all comments on the issue (with pagination and retry)
            const { paginateWithRetry } = require('./.github/scripts/github-api-with-retry.js');
            const comments = await paginateWithRetry(
              github,
              github.rest.issues.listComments,
              {
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                per_page: 100
              }
            );

            // Look for optimizer suggestions marker
            // The optimizer workflow posts a comment containing "Issue Optimization Suggestions"
            // or the hidden marker "ISSUE_OPTIMIZER_SUGGESTIONS"
            const hasOptimizerOutput = comments.some(comment => {
              const body = comment.body || '';
              return body.includes('Issue Optimization Suggestions') ||
                     body.includes('ISSUE_OPTIMIZER_SUGGESTIONS') ||
                     body.includes('## üìã Optimization Suggestions');
            });

            if (!hasOptimizerOutput) {
              // No optimizer output found - this is a bug in the pipeline
              core.warning('‚ö†Ô∏è Optimizer was supposed to run but produced no suggestions comment');

              // Post warning comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                body: `‚ö†Ô∏è **Auto-pilot warning**: Optimizer step completed but no suggestions found.

            This usually means the optimizer workflow was triggered but failed silently.

            **Next steps:**
            1. Check the \`agents-issue-optimizer.yml\` workflow runs
            2. Look for error messages in workflow logs
            3. Remove \`agents:auto-pilot-pause\` and \`needs-human\` labels to retry

            **For maintainers:** Issue paused pending investigation of optimizer failure.`
              });

              // Add pause and needs-human labels
              await github.rest.issues.addLabels({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                labels: ['agents:auto-pilot-pause', 'needs-human']
              });

              core.setFailed('Optimizer produced no output - pausing for investigation');
            } else {
              core.info('‚úÖ Optimizer output found - proceeding with apply step');
            }

      - name: Metrics - Start apply timer
        if: steps.next.outputs.next_step == 'apply'
        env:
          AUTOPILOT_STEP_NAME: apply
          AUTOPILOT_ERROR_CATEGORY: timer-start
        run: |
          python scripts/autopilot_step_timer.py --event start --format epoch-ms --github-env

      - name: Execute step - Apply suggestions (inline)
        if: steps.next.outputs.next_step == 'apply'
        id: apply_step
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
          STEP_COUNT: ${{ steps.cycles.outputs.count }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          ISSUE_NUMBER="$ISSUE_NUMBER"
          STEP_COUNT="$STEP_COUNT"

          # Post progress comment
          gh issue comment "${ISSUE_NUMBER}" --body \
            "ü§ñ **Auto-pilot step $((STEP_COUNT + 1))**: Applying optimization suggestions...

          Running apply inline (not via label trigger)."

          # Get issue body and comments
          gh api "repos/${{ github.repository }}/issues/${ISSUE_NUMBER}" > /tmp/issue.json
          jq -r '.body' /tmp/issue.json > /tmp/issue_body.md
          gh api "repos/${{ github.repository }}/issues/${ISSUE_NUMBER}/comments" \
            --paginate > /tmp/comments.json

          # Extract and apply suggestions
          python -c "
          import json
          import sys
          import re
          sys.path.insert(0, 'scripts/langchain')
          from issue_optimizer import _extract_suggestions_json, apply_suggestions

          with open('/tmp/comments.json') as f:
              comments = json.load(f)

          suggestions = None
          for comment in comments:
              body = comment.get('body', '')
              extracted = _extract_suggestions_json(body)
              if extracted:
                  suggestions = extracted
                  break

          if not suggestions:
              print('ERROR: No suggestions JSON found in comments')
              sys.exit(1)

          # Read current issue body
          with open('/tmp/issue_body.md') as f:
              issue_body = f.read()

          # Apply suggestions
          result = apply_suggestions(issue_body, suggestions, use_llm=False)

          with open('/tmp/updated_body.md', 'w') as f:
              f.write(result['formatted_body'])

          print('Suggestions applied successfully')
          " || exit 1

          # Update issue body
          gh issue edit "${ISSUE_NUMBER}" --body-file /tmp/updated_body.md

          # Add marker label (but don't trigger workflow)
          gh issue edit "${ISSUE_NUMBER}" --add-label "agents:formatted" || true

          echo "‚úÖ Apply complete - continuing to next step"

      - name: Metrics - End apply timer
        if: always() && steps.next.outputs.next_step == 'apply'
        env:
          AUTOPILOT_STEP_NAME: apply
          AUTOPILOT_ERROR_CATEGORY: timer-end
        run: |
          python scripts/autopilot_step_timer.py --event end --format epoch-ms --github-env

      - name: Metrics - Record apply step
        if: always() && steps.next.outputs.next_step == 'apply'
        env:
          AUTOPILOT_STEP_NAME: apply
          AUTOPILOT_ERROR_CATEGORY: metrics-collector
          AUTOPILOT_METRICS_LOG_PATH: ${{ env.AUTOPILOT_METRICS_LOG_PATH }}
        run: |
          success="${{ steps.apply_step.outcome == 'success' }}"
          failure_reason="none"
          if [ "$success" != "true" ]; then
            failure_reason="step-failed"
          fi
          python scripts/autopilot_metrics_collector.py \
            --path "$AUTOPILOT_METRICS_LOG_PATH" \
            --metric-type step \
            --issue-number "${{ steps.context.outputs.issue_number }}" \
            --cycle-count "${{ steps.cycles.outputs.count }}" \
            --step-name "apply" \
            --success "$success" \
            --failure-reason "$failure_reason"

      - name: Metrics - Start capability check timer
        if: steps.next.outputs.next_step == 'capability-check'
        env:
          AUTOPILOT_STEP_NAME: capability-check
          AUTOPILOT_ERROR_CATEGORY: timer-start
        run: |
          python scripts/autopilot_step_timer.py --event start --format epoch-ms --github-env

      - name: Execute step - Capability check & Agent
        if: steps.next.outputs.next_step == 'capability-check'
        id: capability_step
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
          STEP_COUNT: ${{ steps.cycles.outputs.count }}
        with:
          script: |
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);
            const stepCount = parseInt(process.env.STEP_COUNT || '0') + 1;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              body: `ü§ñ **Auto-pilot step ${stepCount}**: Issue prepared! Assigning to agent...

            Adding \`agent:codex\` label. The capability check will run automatically.

            ‚è≥ Agent will create a PR shortly.`
            });

            // Add agent label - capability check triggers on this
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              labels: ['agent:codex']
            });

            // Force-dispatch Codex belt dispatcher to create the branch
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: 'agents-71-codex-belt-dispatcher.yml',
                ref: 'main',
                inputs: {
                  force_issue: issueNumber.toString(),
                  dry_run: 'false'
                }
              });
              core.info(`Dispatched codex belt dispatcher for issue #${issueNumber}`);
            } catch (dispatchError) {
              core.warning(`Could not dispatch codex belt dispatcher: ${dispatchError?.message}`);
            }

      - name: Metrics - End capability check timer
        if: always() && steps.next.outputs.next_step == 'capability-check'
        env:
          AUTOPILOT_STEP_NAME: capability-check
          AUTOPILOT_ERROR_CATEGORY: timer-end
        run: |
          python scripts/autopilot_step_timer.py --event end --format epoch-ms --github-env

      - name: Metrics - Record capability check step
        if: always() && steps.next.outputs.next_step == 'capability-check'
        env:
          AUTOPILOT_STEP_NAME: capability-check
          AUTOPILOT_ERROR_CATEGORY: metrics-collector
          AUTOPILOT_METRICS_LOG_PATH: ${{ env.AUTOPILOT_METRICS_LOG_PATH }}
        run: |
          success="${{ steps.capability_step.outcome == 'success' }}"
          failure_reason="none"
          if [ "$success" != "true" ]; then
            failure_reason="step-failed"
          fi
          python scripts/autopilot_metrics_collector.py \
            --path "$AUTOPILOT_METRICS_LOG_PATH" \
            --metric-type step \
            --issue-number "${{ steps.context.outputs.issue_number }}" \
            --cycle-count "${{ steps.cycles.outputs.count }}" \
            --step-name "capability-check" \
            --success "$success" \
            --failure-reason "$failure_reason"
      - name: Metrics - Start verify timer
        if: steps.next.outputs.next_step == 'verify'
        env:
          AUTOPILOT_STEP_NAME: verify
          AUTOPILOT_ERROR_CATEGORY: timer-start
        run: |
          python scripts/autopilot_step_timer.py --event start --format epoch-ms --github-env

      - name: Execute step - Verify
        if: steps.next.outputs.next_step == 'verify'
        id: verify_step
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
        with:
          script: |
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);

            // Find the merged PR for this issue
            // Look for PRs with the meta marker or explicit closing reference
            const { data: prs } = await github.rest.pulls.list({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'closed',
              sort: 'updated',
              direction: 'desc',
              per_page: 100
            });

            // Helper to detect explicit closing/fixing references to this issue
            function matchesIssueReference(text, num) {
              if (!text) return false;
              // Match: closes #123, fixes #123, resolves #123 (and variations)
              const pattern = new RegExp(
                `\\b(close[sd]?|fixe?[sd]?|resolve[sd]?)\\s+#${num}\\b`, 'i'
              );
              return pattern.test(text);
            }

            // Find PR that references this issue
            const linkedPr = prs.find(pr =>
              pr.merged_at && (
                pr.body?.includes(`meta:issue:${issueNumber}`) ||
                matchesIssueReference(pr.body, issueNumber) ||
                matchesIssueReference(pr.title, issueNumber)
              )
            );

            if (!linkedPr) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                body: `ü§ñ **Auto-pilot**: Issue closed but couldn't find linked merged PR.

            Adding \`verify:evaluate\` label to issue for tracking.`
              });

              await github.rest.issues.addLabels({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                labels: ['verify:evaluate']
              });
              return;
            }

            core.info(`Found merged PR #${linkedPr.number} for issue #${issueNumber}`);

            const verifyMsg = [
              `ü§ñ **Auto-pilot**: Issue closed.`,
              `Triggering verification on PR #${linkedPr.number}...`,
              '',
              'Adding `verify:evaluate` label to PR.'
            ].join('\n');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              body: verifyMsg
            });

            // Add verify label to the merged PR (this triggers agents-verifier.yml)
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: linkedPr.number,
              labels: ['verify:evaluate']
            });

            // Also add to issue for tracking
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              labels: ['verify:evaluate']
            });

      - name: Metrics - End verify timer
        if: always() && steps.next.outputs.next_step == 'verify'
        env:
          AUTOPILOT_STEP_NAME: verify
          AUTOPILOT_ERROR_CATEGORY: timer-end
        run: |
          python scripts/autopilot_step_timer.py --event end --format epoch-ms --github-env

      - name: Metrics - Record verify step
        if: always() && steps.next.outputs.next_step == 'verify'
        env:
          AUTOPILOT_STEP_NAME: verify
          AUTOPILOT_ERROR_CATEGORY: metrics-collector
          AUTOPILOT_METRICS_LOG_PATH: ${{ env.AUTOPILOT_METRICS_LOG_PATH }}
        run: |
          success="${{ steps.verify_step.outcome == 'success' }}"
          failure_reason="none"
          if [ "$success" != "true" ]; then
            failure_reason="step-failed"
          fi
          python scripts/autopilot_metrics_collector.py \
            --path "$AUTOPILOT_METRICS_LOG_PATH" \
            --metric-type step \
            --issue-number "${{ steps.context.outputs.issue_number }}" \
            --cycle-count "${{ steps.cycles.outputs.count }}" \
            --step-name "verify" \
            --success "$success" \
            --failure-reason "$failure_reason"

      - name: Metrics - Start create-pr timer
        if: steps.next.outputs.next_step == 'create-pr'
        env:
          AUTOPILOT_STEP_NAME: create-pr
          AUTOPILOT_ERROR_CATEGORY: timer-start
        run: |
          python scripts/autopilot_step_timer.py --event start --format epoch-ms --github-env

      - name: Execute step - Create PR
        if: steps.next.outputs.next_step == 'create-pr'
        id: create_pr_step
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
          ISSUE_TITLE: ${{ steps.context.outputs.issue_title }}
          STEP_COUNT: ${{ steps.cycles.outputs.count }}
        with:
          script: |
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);
            const issueTitle = process.env.ISSUE_TITLE || `Issue #${issueNumber}`;
            const stepCount = parseInt(process.env.STEP_COUNT || '0') + 1;
            const branchName = `codex/issue-${issueNumber}`;

            // Check if branch exists
            let branchExists = false;
            try {
              await github.rest.repos.getBranch({
                owner: context.repo.owner,
                repo: context.repo.repo,
                branch: branchName
              });
              branchExists = true;
              core.info(`Branch ${branchName} exists`);
            } catch (e) {
              if (e.status === 404) {
                core.info(`Branch ${branchName} does not exist yet`);
              } else {
                throw e;
              }
            }

            if (!branchExists) {
              // Branch not created yet - agent still working
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                body: `ü§ñ **Auto-pilot step ${stepCount}**: Waiting for agent to create branch...

            The agent has been assigned but hasn't created the branch yet.
            Branch expected: \`${branchName}\`

            ‚è≥ Auto-pilot will check again on next trigger.`
              });
              return;
            }

            const { data: repoInfo } = await github.rest.repos.get({
              owner: context.repo.owner,
              repo: context.repo.repo
            });
            const baseBranch = repoInfo.default_branch || 'main';

            // If branch has no commits ahead of base, dispatch the belt worker
            try {
              const { data: comparison } = await github.rest.repos.compareCommits({
                owner: context.repo.owner,
                repo: context.repo.repo,
                base: baseBranch,
                head: branchName
              });

              if ((comparison.ahead_by || 0) === 0) {
                core.info(`Branch ${branchName} has no commits ahead of ${baseBranch}`);
                let workerDispatched = false;
                try {
                  await github.rest.actions.createWorkflowDispatch({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    workflow_id: 'agents-72-codex-belt-worker-dispatch.yml',
                    ref: baseBranch,
                    inputs: {
                      issue: issueNumber.toString(),
                      branch: branchName,
                      base: baseBranch,
                      source: 'auto-pilot'
                    }
                  });
                  workerDispatched = true;
                  core.info(`Dispatched codex belt worker for issue #${issueNumber}`);
                } catch (dispatchError) {
                  core.warning(`Could not dispatch codex belt worker: ${dispatchError?.message}`);
                }

                const dispatchLine = workerDispatched
                  ? 'Dispatched the Codex belt worker to generate changes.'
                  : 'Failed to dispatch the Codex belt worker. Please check workflow permissions.';

                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issueNumber,
                  body: `ü§ñ **Auto-pilot step ${stepCount}**: Branch ready, waiting for commits

            Branch \`${branchName}\` exists but has no commits yet.
            ${dispatchLine}

            ‚è≥ Auto-pilot will check again on next trigger.`
                });
                return;
              }
            } catch (compareError) {
              const message = compareError?.message || String(compareError);
              core.warning(`Could not compare ${branchName} to ${baseBranch}: ${message}`);
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                body: `ü§ñ **Auto-pilot step ${stepCount}**: Compare failed

            Could not compare \`${branchName}\` to \`${baseBranch}\`.
            Skipping PR creation for now. Auto-pilot will retry on the next trigger.`
              });
              return;
            }

            // Branch has commits - create PR
            core.info(`Creating PR from ${branchName}`);

            // Fetch issue body to include in PR
            const { data: issue } = await github.rest.issues.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber
            });

            const prTitle = `[Auto-pilot] ${issueTitle}`;
            const prBody = [
              `<!-- meta:issue:${issueNumber} -->`,
              '',
              `Closes #${issueNumber}`,
              '',
              issue.body || ''
            ].join('\n');

            try {
              const { data: pr } = await github.rest.pulls.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: prTitle,
                head: branchName,
                base: baseBranch,
                body: prBody
              });

              core.info(`Created PR #${pr.number}`);

              // Add standard agent labels to the PR (separate try-catch to not fail PR creation)
              let labelsAdded = false;
              try {
                await github.rest.issues.addLabels({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: pr.number,
                  labels: ['agent:codex', 'agents:keepalive', 'autofix']
                });
                labelsAdded = true;
                core.info(`Added agent labels to PR #${pr.number}`);
              } catch (labelError) {
                const errMsg = labelError?.message || String(labelError);
                core.warning(`Failed to add labels to PR #${pr.number}: ${errMsg}`);
              }

              const labelStatus = labelsAdded
                ? '‚úÖ Added labels: `agent:codex`, `agents:keepalive`, `autofix`'
                : '‚ö†Ô∏è Could not add labels (add manually)';

              // Dispatch PR meta workflow to build Automated Status Summary
              // (GITHUB_TOKEN actions do not trigger workflow runs automatically)
              try {
                await github.rest.actions.createWorkflowDispatch({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: 'agents-pr-meta-v4.yml',
                  ref: 'main',
                  inputs: {
                    pr_number: pr.number.toString(),
                    debug: 'false'
                  }
                });
                core.info(`Dispatched PR meta update for PR #${pr.number}`);
              } catch (dispatchError) {
                core.warning(`Could not dispatch PR meta update: ${dispatchError?.message}`);
              }

              // Dispatch keepalive workflow since GITHUB_TOKEN labels don't trigger it
              try {
                await github.rest.actions.createWorkflowDispatch({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: 'agents-keepalive-loop.yml',
                  ref: 'main',
                  inputs: {
                    pr_number: pr.number.toString(),
                    force_retry: 'true'
                  }
                });
                core.info(`Dispatched keepalive for PR #${pr.number}`);
              } catch (dispatchError) {
                core.warning(`Could not dispatch keepalive: ${dispatchError?.message}`);
              }

              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                body: `ü§ñ **Auto-pilot step ${stepCount}**: PR created!

            ‚úÖ Created PR #${pr.number} from branch \`${branchName}\`
            ${labelStatus}

            The PR will now go through CI checks. Auto-pilot will continue monitoring.`
              });

            } catch (e) {
              if (e.status === 422 && e.message?.includes('already exists')) {
                core.info('PR already exists - this is fine');
              } else {
                // PR creation failed - report but don't fail workflow
                core.warning(`Failed to create PR: ${e.message}`);
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issueNumber,
                  body: `ü§ñ **Auto-pilot step ${stepCount}**: Could not create PR

            ‚ö†Ô∏è Branch \`${branchName}\` exists but PR creation failed.

            Error: ${e.message}

            Please create the PR manually or check permissions.`
                });
              }
            }

      - name: Metrics - End create-pr timer
        if: always() && steps.next.outputs.next_step == 'create-pr'
        env:
          AUTOPILOT_STEP_NAME: create-pr
          AUTOPILOT_ERROR_CATEGORY: timer-end
        run: |
          python scripts/autopilot_step_timer.py --event end --format epoch-ms --github-env

      - name: Metrics - Record create-pr step
        if: always() && steps.next.outputs.next_step == 'create-pr'
        env:
          AUTOPILOT_STEP_NAME: create-pr
          AUTOPILOT_ERROR_CATEGORY: metrics-collector
          AUTOPILOT_METRICS_LOG_PATH: ${{ env.AUTOPILOT_METRICS_LOG_PATH }}
        run: |
          success="${{ steps.create_pr_step.outcome == 'success' }}"
          failure_reason="none"
          if [ "$success" != "true" ]; then
            failure_reason="step-failed"
          fi
          python scripts/autopilot_metrics_collector.py \
            --path "$AUTOPILOT_METRICS_LOG_PATH" \
            --metric-type step \
            --issue-number "${{ steps.context.outputs.issue_number }}" \
            --cycle-count "${{ steps.cycles.outputs.count }}" \
            --step-name "create-pr" \
            --success "$success" \
            --failure-reason "$failure_reason"

      - name: Metrics - Start monitor-pr timer
        if: steps.next.outputs.next_step == 'monitor-pr'
        env:
          AUTOPILOT_STEP_NAME: monitor-pr
          AUTOPILOT_ERROR_CATEGORY: timer-start
        run: |
          python scripts/autopilot_step_timer.py --event start --format epoch-ms --github-env

      - name: Report - Monitoring PR
        if: steps.next.outputs.next_step == 'monitor-pr'
        id: monitor_pr_step
        uses: actions/github-script@v8
        with:
          script: |
            const prNumber = '${{ steps.context.outputs.linked_pr }}';
            core.info(`PR #${prNumber} exists. Keepalive and autofix will handle CI.`);

      - name: Metrics - End monitor-pr timer
        if: always() && steps.next.outputs.next_step == 'monitor-pr'
        env:
          AUTOPILOT_STEP_NAME: monitor-pr
          AUTOPILOT_ERROR_CATEGORY: timer-end
        run: |
          python scripts/autopilot_step_timer.py --event end --format epoch-ms --github-env

      - name: Metrics - Record monitor-pr step
        if: always() && steps.next.outputs.next_step == 'monitor-pr'
        env:
          AUTOPILOT_STEP_NAME: monitor-pr
          AUTOPILOT_ERROR_CATEGORY: metrics-collector
          AUTOPILOT_METRICS_LOG_PATH: ${{ env.AUTOPILOT_METRICS_LOG_PATH }}
        run: |
          success="${{ steps.monitor_pr_step.outcome == 'success' }}"
          failure_reason="none"
          if [ "$success" != "true" ]; then
            failure_reason="step-failed"
          fi
          python scripts/autopilot_metrics_collector.py \
            --path "$AUTOPILOT_METRICS_LOG_PATH" \
            --metric-type step \
            --issue-number "${{ steps.context.outputs.issue_number }}" \
            --cycle-count "${{ steps.cycles.outputs.count }}" \
            --step-name "monitor-pr" \
            --success "$success" \
            --failure-reason "$failure_reason"

      - name: Metrics - Start check-completion timer
        if: steps.next.outputs.next_step == 'check-completion'
        env:
          AUTOPILOT_STEP_NAME: check-completion
          AUTOPILOT_ERROR_CATEGORY: timer-start
        run: |
          python scripts/autopilot_step_timer.py --event start --format epoch-ms --github-env

      - name: Execute step - Check Completion & Trigger Merge
        if: steps.next.outputs.next_step == 'check-completion'
        id: completion_step
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
          PR_NUMBER: ${{ steps.context.outputs.linked_pr }}
          STEP_COUNT: ${{ steps.cycles.outputs.count }}
        with:
          script: |
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);
            const prNumber = parseInt(process.env.PR_NUMBER);
            const stepCount = parseInt(process.env.STEP_COUNT || '0') + 1;

            // Get PR details to check CI status
            const { data: pr } = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: prNumber
            });

            // Check if PR is mergeable
            if (pr.mergeable !== true) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                body: `ü§ñ **Auto-pilot step ${stepCount}**: PR completion check

            ‚ö†Ô∏è PR #${prNumber} is not mergeable or its mergeability is still being computed.

            Auto-pilot cannot proceed with merge. Manual intervention may be required.`
              });
              return;
            }

            // Check CI status
            let checksPass = true;
            try {
              const { data: checks } = await github.rest.checks.listForRef({
                owner: context.repo.owner,
                repo: context.repo.repo,
                ref: pr.head.sha
              });

              const pendingChecks = checks.check_runs.filter(check =>
                check.status !== 'completed'
              );
              const failedChecks = checks.check_runs.filter(check =>
                check.status === 'completed' && check.conclusion !== 'success' &&
                check.conclusion !== 'neutral' && check.conclusion !== 'skipped'
              );

              if (failedChecks.length > 0 || pendingChecks.length > 0) {
                checksPass = false;
              }
            } catch (e) {
              core.warning(`Could not check CI status: ${e.message}`);
            }

            if (!checksPass) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                body: `ü§ñ **Auto-pilot step ${stepCount}**: PR completion check

            ‚ö†Ô∏è PR #${prNumber} has failing CI checks.

            Auto-pilot will wait for checks to pass before merging.`
              });
              return;
            }

            // All checks pass - add automerge label
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              labels: ['automerge']
            });

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              body: `ü§ñ **Auto-pilot step ${stepCount}**: PR ready for merge!

            ‚úÖ All tasks completed
            ‚úÖ CI checks passing
            ‚úÖ No merge conflicts

            Added \`automerge\` label to PR #${prNumber}. The orchestrator will merge it shortly.`
            });

            core.info(`PR #${prNumber} ready for automerge - added label`);

      - name: Metrics - End check-completion timer
        if: always() && steps.next.outputs.next_step == 'check-completion'
        env:
          AUTOPILOT_STEP_NAME: check-completion
          AUTOPILOT_ERROR_CATEGORY: timer-end
        run: |
          python scripts/autopilot_step_timer.py --event end --format epoch-ms --github-env

      - name: Metrics - Record check-completion step
        if: always() && steps.next.outputs.next_step == 'check-completion'
        env:
          AUTOPILOT_STEP_NAME: check-completion
          AUTOPILOT_ERROR_CATEGORY: metrics-collector
          AUTOPILOT_METRICS_LOG_PATH: ${{ env.AUTOPILOT_METRICS_LOG_PATH }}
        run: |
          success="${{ steps.completion_step.outcome == 'success' }}"
          failure_reason="none"
          if [ "$success" != "true" ]; then
            failure_reason="step-failed"
          fi
          python scripts/autopilot_metrics_collector.py \
            --path "$AUTOPILOT_METRICS_LOG_PATH" \
            --metric-type step \
            --issue-number "${{ steps.context.outputs.issue_number }}" \
            --cycle-count "${{ steps.cycles.outputs.count }}" \
            --step-name "check-completion" \
            --success "$success" \
            --failure-reason "$failure_reason"

      # Re-dispatch workflow to continue pipeline after prep steps
      # GitHub prevents recursive triggers on labels added by GITHUB_TOKEN
      - name: Re-dispatch for next step
        if: |
          steps.context.outputs.should_continue == 'true' &&
          steps.cycles.outputs.exceeded != 'true' &&
          contains(
            fromJSON('["format","optimize","apply","capability-check","create-pr","monitor-pr"]'),
            steps.next.outputs.next_step
          )
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
          CURRENT_STEP: ${{ steps.next.outputs.next_step }}
        with:
          script: |
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);
            const currentStep = process.env.CURRENT_STEP;

            // Determine next step explicitly to avoid label race conditions
            // Don't rely on 'auto' detection which has API cache timing issues
            const nextStepMap = {
              'format': 'optimize',
              'optimize': 'apply',
              'apply': 'capability-check',
              'capability-check': 'auto',  // Let it detect agent/PR state
              'create-pr': 'auto',  // Re-evaluate after branch/PR creation
              'monitor-pr': 'auto'  // Let it check PR completion
            };
            const nextStep = nextStepMap[currentStep] || 'auto';

            // For monitor-pr or create-pr, add delay to avoid tight polling loops
            // Keepalive updates and branch creation can take time; without delay
            // we'd spam Actions with rapid re-dispatches
            if (currentStep === 'monitor-pr' || currentStep === 'create-pr') {
              const waitTime = currentStep === 'monitor-pr' ? 120000 : 60000;
              core.info(`Waiting ${waitTime/1000}s before re-dispatch...`);
              await new Promise(resolve => setTimeout(resolve, waitTime));
            }
            // Other steps dispatch immediately - explicit force_step
            // eliminates label dependency, no wait needed

            core.info(
              `Re-dispatching auto-pilot: issue #${issueNumber}, ` +
              `next_step=${nextStep}`
            );
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'agents-auto-pilot.yml',
              ref: 'main',
              inputs: {
                issue_number: issueNumber.toString(),
                force_step: nextStep
              }
            });

            core.info(
              `Re-dispatched for issue #${issueNumber} after ` +
              `${currentStep} step (next: ${nextStep})`
            );

      - name: Report - Done
        if: steps.next.outputs.next_step == 'done'
        uses: actions/github-script@v8
        env:
          ISSUE_NUMBER: ${{ steps.context.outputs.issue_number }}
        with:
          script: |
            const issueNumber = parseInt(process.env.ISSUE_NUMBER);

            // Remove auto-pilot label since we're done
            try {
              await github.rest.issues.removeLabel({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issueNumber,
                name: 'agents:auto-pilot'
              });
            } catch (e) {
              // Label might already be removed (404) - that's OK
              if (e && e.status === 404) {
                core.info('Auto-pilot label already removed or not found');
              } else {
                core.warning(`Unexpected error removing auto-pilot label: ${e?.message || e}`);
              }
            }

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issueNumber,
              body: `## ‚úÖ Auto-Pilot Complete

            This issue has been fully processed:
            - ‚úÖ Issue formatted and optimized
            - ‚úÖ Agent assigned and PR created
            - ‚úÖ PR merged
            - ‚úÖ Verification triggered

            Thank you for using auto-pilot! üöÄ`
            });

            core.info('Auto-pilot complete!');
